# [Batch Normalization in Deep Neural Networks](http://sahilsingh.org/batch-normalization-in-deep-neural-networks.html)

> Batch Normalization is a straightforward way for optimizing the training of deep neural networks.
It is based on the idea that inputs to all layers of a neural network should be whitened
- i.e. linearly transformed to have zero mean and unit variance, before being fed into the activation function.